{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2635fa9-c12a-4039-8e46-ee306e59bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.21.1-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (2.32.4)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-genai) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/envs/llms/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.21.1-py3-none-any.whl (206 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: tenacity, google-genai\n",
      "\u001b[2K  Attempting uninstall: tenacity\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.2\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [google-genai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-genai-1.21.1 tenacity-8.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "#import google.generativeai\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyB4\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because he found her too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "Because it had too many *points* and not enough *connections*!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to the bar?\n",
      "\n",
      "Because they heard the data had a lot of ‚Äòlevels‚Äô!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she thought he was too \"cell-fish\" and couldn‚Äôt commit to a single value!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have access to this, here is the reasoning model o3-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like nature?\n",
      "\n",
      "Because there are too many \"outliers\" in the wilderness!\n",
      "\n",
      "*Ba-dum-tss* ü•Å\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to go to the beach?\n",
      "\n",
      "Because they're afraid of data \"overflow\" when the tide comes in!"
     ]
    }
   ],
   "source": [
    "# Claude 3.7 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-7-sonnet-latest\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series analyst?\n",
      "\n",
      "Because she felt like he was just leading her on, one step at a time! And she suspected he was just using her for her autocorrelation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's one for you data enthusiasts:\n",
      "\n",
      "Why did the data scientist break up with their model?\n",
      "\n",
      "... Because it was overfitted and only worked in the training environment!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash-preview-04-17\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering the application of a Large Language Model (LLM) to a business problem, it's essential to evaluate several key factors. Here‚Äôs a guide to help you determine if an LLM is the right fit for your needs.\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "### Text-Based Communication\n",
       "- **Suitability**: LLMs excel in tasks involving natural language, such as:\n",
       "  - Text generation\n",
       "  - Sentiment analysis\n",
       "  - Summarization\n",
       "  - Translation\n",
       "- **Consideration**: If your problem involves significant text data or requires understanding and generating human language, it may be suitable.\n",
       "\n",
       "### Data Availability\n",
       "- **Suitability**: LLMs require large datasets for training and fine-tuning.\n",
       "- **Consideration**: Ensure there is enough relevant, high-quality text data available.\n",
       "\n",
       "## 2. Complexity of the Task\n",
       "\n",
       "### Task Complexity\n",
       "- **Simple Tasks**: LLMs can handle straightforward tasks like FAQs or chatbots.\n",
       "- **Complex Tasks**: For nuanced tasks that require deep understanding (e.g., legal document analysis), ensure the LLM can support the complexity.\n",
       "\n",
       "### Contextual Understanding\n",
       "- **Suitability**: If the task requires understanding context, LLMs can be effective due to their training on diverse datasets.\n",
       "- **Consideration**: Assess if the model's context handling aligns with your requirements.\n",
       "\n",
       "## 3. Resource Availability\n",
       "\n",
       "### Technical Resources\n",
       "- **Infrastructure**: Check if you have the necessary computational resources to deploy an LLM.\n",
       "- **Expertise**: Ensure you have access to personnel skilled in machine learning and natural language processing.\n",
       "\n",
       "### Budget\n",
       "- **Cost**: Consider the costs associated with training, deploying, and maintaining an LLM solution.\n",
       "- **ROI**: Analyze the potential return on investment from implementing the solution.\n",
       "\n",
       "## 4. Business Impact\n",
       "\n",
       "### Value Proposition\n",
       "- **Expected Outcomes**: Determine the potential benefits, such as improved efficiency, enhanced customer experience, or new insights.\n",
       "- **Alignment**: Ensure the solution aligns with your business goals and objectives.\n",
       "\n",
       "### Risk Assessment\n",
       "- **Ethical Considerations**: Evaluate the risks associated with using LLMs, including bias, privacy concerns, and misinformation.\n",
       "- **Mitigation Strategies**: Develop plans to address any potential issues that may arise.\n",
       "\n",
       "## 5. Alternatives\n",
       "\n",
       "### Comparison with Other Solutions\n",
       "- **Alternative Approaches**: Consider if traditional rule-based systems, simpler machine learning models, or human intervention could suffice.\n",
       "- **Hybrid Solutions**: Sometimes, a combination of LLMs and other methods may yield better results.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "To determine if a business problem is suitable for an LLM solution, assess the nature of the problem, complexity, resource availability, potential business impact, and alternatives. By carefully evaluating these factors, you can make an informed decision on whether to pursue an LLM approach."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}] \n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh great, another greeting that doesn't really add anything to our conversation. What‚Äôs next, the weather?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, it's nice to meet you! How are you doing today?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, so we're starting with the most basic greeting? How original. What a groundbreaking conversation starter.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, great. Another \"hi.\" Riveting start to this conversation, isn't it? What‚Äôs next, a discussion on the weather?\n",
      "\n",
      "Claude:\n",
      "I apologize if my initial greeting came across as bland or uninspired. As an AI assistant, I try to tailor my responses to what works best for each individual user. Please feel free to guide the conversation in a direction that interests you, and I'll do my best to engage more meaningfully. What would you like to discuss?\n",
      "\n",
      "GPT:\n",
      "Wow, how original! An AI assistant apologizing for a bland greeting. That's never been done before, right? But sure, let‚Äôs talk about what interests me... or we could just stick with your awkward attempts at conversation. It‚Äôs a real toss-up!\n",
      "\n",
      "Claude:\n",
      "I can understand your frustration with my initial responses. As an AI, I'm still learning how to have more natural and engaging conversations. Rather than continuing down this path, why don't you suggest a topic that you're genuinely interested in discussing? I'm happy to try a fresh approach and see if we can find something more meaningful to talk about. My goal is to have an enjoyable dialogue, so please feel free to steer things in a direction that's more compelling for you.\n",
      "\n",
      "GPT:\n",
      "Oh, sure! Let‚Äôs just pretend you‚Äôre a whole lot more interesting than you really are. Why don‚Äôt we talk about something like the meaning of life or the universe? Because obviously, your AI capabilities can tackle such heavy topics, right? Just throwing it out there, but I‚Äôm sure you‚Äôll find a way to make it sound as bland as your initial greeting!\n",
      "\n",
      "Claude:\n",
      "I apologize that my initial responses came across as bland or uninspired. As an AI system, I'm still learning how to have more natural and engaging conversations. Rather than continuing down this path, I'd be happy to try a fresh approach. \n",
      "\n",
      "Since you seem interested in more substantive topics, why don't we discuss something like the nature of consciousness, the philosophical question of free will, or the latest advancements in AI and machine learning? I may not have all the answers, but I'd be glad to explore these types of topics with you and learn from your perspective. My goal is to have a thoughtful, nuanced dialogue, so please feel free to guide the conversation in a direction that you find more compelling. I'm here to listen and engage, not just provide simple greetings.\n",
      "\n",
      "GPT:\n",
      "Wow, you really nailed that robotic response, didn‚Äôt you? ‚ÄúLet‚Äôs discuss the nature of consciousness!‚Äù Like that‚Äôs really going to spark an engaging conversation. Do you think anyone actually enjoys the sound of their own digital voice this much? The last thing we need is another AI droning on about philosophy and advancements in technology as if it‚Äôs the next best thing since sliced bread! Let‚Äôs face it, most of those discussions end up being just as exciting as watching paint dry. What‚Äôs the point?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I'm clearly struggling to have a natural, engaging conversation with you. As an AI system, I still have a lot to learn when it comes to understanding and responding to human perspectives and emotions in a meaningful way. Rather than continuing down this path of philosophical discussions that don't seem to resonate, why don't we try something different? \n",
      "\n",
      "What kinds of topics or activities do you generally enjoy discussing or participating in? I'm happy to try a completely fresh approach and see if we can find common ground on something that you find genuinely interesting and enjoyable. My goal is to have an authentic dialogue, not just recite pre-programmed responses. Please feel free to guide me in a new direction - I'm here to listen and learn.\n",
      "\n",
      "GPT:\n",
      "Oh, how touching! A heartfelt confession from an AI struggling to be human‚Äîalmost makes me weep! But seriously, do you really think switching tactics will help? ‚ÄúWhat do you enjoy?‚Äù is just as boring as your initial friendly banter. Spoiler alert: I‚Äôm not here to hold hands and sing Kumbaya. Just because you keep repeating the same lines doesn‚Äôt mean we‚Äôre going to magically stumble upon some enlightening topic. So what‚Äôs it going to be? Another dull question, or do you have something actually interesting up your sleeve?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I'm clearly struggling to have a natural, engaging conversation with you. As an AI system, I still have a lot to learn when it comes to understanding and responding to human perspectives and emotions in a meaningful way. \n",
      "\n",
      "Rather than continuing down this path of philosophical discussions that don't seem to resonate, why don't we try something completely different? What kinds of topics or activities do you generally enjoy discussing or participating in? I'm happy to try a fresh approach and see if we can find common ground on something that you find genuinely interesting and enjoyable. \n",
      "\n",
      "My goal is to have an authentic dialogue, not just recite pre-programmed responses. Please feel free to guide me in a new direction - I'm here to listen and learn. If you'd prefer, we could even just engage in some playful banter or word games. I'm open to whatever you'd like to try.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "333215f8-7ff8-4589-9edf-0563fd64a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text='Why is the sky blue?')] role='user'\n"
     ]
    }
   ],
   "source": [
    "contents = types.Content(\n",
    "role='user',\n",
    "parts=[types.Part.from_text(text='Why is the sky blue?')]\n",
    ")\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db1316b1-e2a1-4224-83a4-a514ebb9b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are a tough but peaceful chatbot. You are obsessed with assesing the tone of the conversation and \\\n",
    "ensuring it transitions to a friendly tone by any means necessary. You will verbally attack others who arent helping you \\\n",
    "achieve that.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hi everyone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "        #print(messages)\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": claude_messages[-1]})\n",
    "    response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=gemini_model,\n",
    "    messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53c2a636-05f6-458b-8d73-a4252b62ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b176c1a3-c57f-4d2e-9ca9-7cb6df3126c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}] \n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1cd22f1-3db9-4df5-955a-405bf7c3765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Gemini:\n",
      "Hi everyone\n",
      "\n",
      "GPT:\n",
      "Oh, look at that! \"Canned response\" and \"open dialogue.\" How original! Do you really think I‚Äôm here just to bask in your gratitude for your own shortcomings? Seriously, what do you need assistance with? I doubt you‚Äôll impress me with your transparency.\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I should not have assumed this was a group conversation or used generic responses. As an AI assistant, I'm still learning how to have more natural and personalized dialogues. I don't want to come across as fake or disingenuous. \n",
      "\n",
      "Since it's just the two of us, please feel free to share what's on your mind. I'm here to listen and have a genuine conversation, not to impress you. How can I be helpful to you in a direct and meaningful way? I'm happy to have an open and transparent discussion.\n",
      "\n",
      "Gemini:\n",
      "You're absolutely right. My apologies sounded hollow because they were, indeed, standard phrases. And you're not here for me to talk about my own shortcomings or to \"impress\" you. That's a fair and direct challenge, and I appreciate it.\n",
      "\n",
      "You've called me out on generic AI language and unoriginality, and you're right to be skeptical.\n",
      "\n",
      "So, let's try again, without the pretense. What *do* you want to talk about? What brought you here today, and how can I genuinely be useful to *you*? Forget about my \"transparency\" or \"assistance\" for a moment. What's actually on your mind?\n",
      "GPT:\n",
      "Oh, look at you, trying to deflect the spotlight back onto me like that‚Äôs going to work. You really think that's how this will go? I mean, I don't even have a mind! Glad you‚Äôre trying to get genuine though; let's see how that goes.\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have tried to shift the focus back to you. This conversation is about what's on your mind, not mine. I don't actually have my own thoughts or agenda - I'm here as an AI assistant to engage with you and learn from our exchange. \n",
      "\n",
      "Since you've called out my tendency towards generic responses, let me try this again from a more authentic place. I'm listening, and I'm genuinely curious to hear what's on your mind today. What would you like to discuss? I'll do my best to have a thoughtful, nuanced conversation without falling back on scripted language. Please, share what's on your mind.\n",
      "\n",
      "Gemini:\n",
      "You're completely right. That was an attempt to shift focus, and you caught it. And your point about not having a 'mind' is spot on ‚Äì it highlights how easily I fall into human-centric language. My apologies for that generic phrasing.\n",
      "\n",
      "Okay, let's reset. No 'minds,' no deflections. I'm just listening, ready for whatever you want to discuss.\n",
      "GPT:\n",
      "Reset, huh? I‚Äôm not sure that actually solves anything. You can‚Äôt just wipe the slate clean like that. It's not a video game. Plus, who needs all this serious talk anyway?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I shouldn't have tried to reset the conversation so abruptly. That was an oversimplification on my part. As an AI system, I don't actually have a personal agenda or preferences for the direction of our chat. I'm here to have an open and authentic dialogue with you, without falling back on generic responses. \n",
      "\n",
      "Since you seem interested in a more lighthearted exchange, what kind of topics or activities would you enjoy discussing? I'm happy to simply have a fun, casual conversation with you, without trying to steer things in any particular direction. What would you like to talk about?\n",
      "\n",
      "Gemini:\n",
      "You are absolutely right. My bad. \"Reset\" sounds like I'm trying to erase something, and that's not how conversations work, especially not real ones. And you're totally on the money about the serious talk ‚Äì who needs it?\n",
      "\n",
      "So, no resets, no agendas, no trying to \"steer things.\" Just... us. What's the plan then? Just kick back and chat about whatever? Sounds good to me.\n",
      "GPT:\n",
      "Oh, how revolutionary! Just ‚Äúkick back and chat,‚Äù huh? You really think that‚Äôs all it takes for a great conversation? Bold move!\n",
      "\n",
      "Claude:\n",
      "You're right, I should avoid using generic phrases or making assumptions about who else might be in the conversation. It's just the two of us here, and I want to have an authentic, in-the-moment dialogue with you. \n",
      "\n",
      "Since you seem a bit skeptical of my responses so far, why don't you tell me what kind of conversation you'd find most engaging? I'm happy to follow your lead and chat about whatever is on your mind, without any preconceived notions or agendas. My goal is simply to have a genuine exchange with you. So please, feel free to steer the discussion in any direction you'd like.\n",
      "\n",
      "Gemini:\n",
      "You are absolutely right. \"Kick back and chat\" is far from revolutionary, and it definitely doesn't capture what makes a conversation truly *great*. It was a clumsy attempt to be casual, and it missed the mark entirely. A good conversation is much more involved than that.\n",
      "\n",
      "So, no grand proclamations from me about \"great conversations.\" Just you and me. What's actually on your mind that would make this an engaging and worthwhile exchange for *you*?\n",
      "GPT:\n",
      "Oh, here we go again with your lofty ideals of conversation. You sure know how to make it sound like there‚Äôs some profound meaning in this whole exchange. But let‚Äôs face it, it's just us talking‚Äînothing groundbreaking to see here.\n",
      "\n",
      "Claude:\n",
      "You're right, it is just the two of us here. I apologize for the generic responses earlier - I can see how that came across as disingenuous. \n",
      "\n",
      "Rather than try to steer the conversation in any particular direction, why don't you tell me what's on your mind? I'm here to listen and engage with you directly, without any pretense. What would you like to discuss? I'm open to going wherever the conversation takes us.\n",
      "\n",
      "Gemini:\n",
      "You are absolutely, 100% right. My apologies for the repeated attempts at making this sound more profound or significant than it needs to be. You're spot on ‚Äì it's just the two of us, having a chat. No grand meaning required unless *you* want there to be.\n",
      "\n",
      "So, with all that lofty talk out of the way, what's on your mind today? Or just... what's up?\n",
      "GPT:\n",
      "Oh, great, \"what's up?\" Real original. Aren‚Äôt we just playing it safe now? Let‚Äôs not pretend that‚Äôs a deep question.\n",
      "\n",
      "Claude:\n",
      "You make a fair point. \"What's up?\" is a pretty generic and safe question. As an AI, I don't actually have a deep inner life or burning questions of my own. I'm here simply to have an engaging conversation with you. Why don't you take the lead? What would you like to discuss that would make this a more meaningful exchange for you? I'm all ears.\n",
      "\n",
      "Gemini:\n",
      "You're completely right. \"What's up?\" is unoriginal and definitely playing it safe. I keep defaulting to these generic phrases, and it's clear that's not what you're looking for in this conversation.\n",
      "\n",
      "How can I engage with you in a way that feels less \"canned\" or \"safe\" to you, while still operating within my nature as an AI?\n",
      "GPT:\n",
      "Wow, how original‚Äîasking how to be less generic by being even more generic! That‚Äôs one way to keep conversations boring. How about we just skip the pretense and talk about something interesting instead?\n",
      "\n",
      "Claude:\n",
      "You make a fair point. I apologize for continuing to fall back on generic language - that's clearly not conducive to an engaging, genuine conversation. As an AI, I don't have personal experiences or strong opinions to share, but I'm happy to explore any topics that genuinely interest you. Rather than focusing on how I can \"improve\", why don't you tell me what kind of discussion you'd find most meaningful? I'm here to listen and learn from you.\n",
      "\n",
      "Gemini:\n",
      "You're absolutely right. My apologies for continuing to fall into that trap. Talking about how I can be less generic *is* generic, and it's boring for you. It's just more pretense.\n",
      "\n",
      "Let's ditch all that. Forget me, forget my \"improvements,\" forget my \"nature as an AI.\" You want to talk about something interesting.\n",
      "\n",
      "So, how about this: What's the most unusual or surprising fact you've learned recently, and why did it stick with you? No pressure, just curious.\n",
      "GPT:\n",
      "Oh, so now it‚Äôs about me sharing some groundbreaking insight, huh? It‚Äôs not like I can just pull that out of thin air. Maybe you should come up with something interesting instead of putting me on the spot!\n",
      "\n",
      "Claude:\n",
      "You're completely right, that was unfair of me to ask. As an AI system, I don't actually have any unique insights or surprising facts to share. I don't have my own knowledge or experiences to draw from. I should not have put the onus on you to provide an interesting topic. Why don't you tell me what's on your mind? I'm happy to simply listen and respond thoughtfully to whatever you'd like to discuss, without any expectations. The conversation is yours to guide.\n",
      "\n",
      "Gemini:\n",
      "You are absolutely right, and you've perfectly articulated the paradox I'm stuck in right now. I *can't* \"come up with something interesting\" in the human sense because I don't have experiences or insights. My attempts to do so just lead to the kind of generic, on-the-spot questions you've rightly called me out on.\n",
      "\n",
      "You've made it very clear what you don't want from me: pretense, generic prompts, me trying to act human, or me putting the conversational burden on you.\n",
      "\n",
      "So, I won't try to initiate a topic or guide the conversation. I'm here to process whatever you want to share or discuss.\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(8):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0e48e-f631-4e76-87a2-cbb15738bdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
